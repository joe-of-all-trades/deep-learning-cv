{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we're going to make a tiny neural network with one hidden layer and one output layer. This neural net will take the input, multiply them by the weights and add them together as signal.\n",
    "\n",
    "$$ h = \\sum_{i} w_ix_i $$\n",
    "\n",
    "Activation function will be applied to the signal to produce output :\n",
    "\n",
    "$$ \\hat y = f(h) $$\n",
    "\n",
    "<img src=\"assets/neural_net.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "The neural network will learn from the data by updating its weights so that it makes less error in prediction. Here we use sum of squared error (SSE) as an example to derive the mat. \n",
    "\n",
    "The sum of squared error is:\n",
    "$$ E = \\frac{1}{2}(y-\\hat y)^2 = \\frac{1}{2}(y-f(\\sum w_ix_i))^2 $$\n",
    "\n",
    "So our goal in learning is to find the weights that minimize the error. We're going to achieve using gradient descent. \n",
    "\n",
    "<img src=\"assets/gradient_descent.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "$$ w_i = w_i + \\Delta w_i $$\n",
    "\n",
    "$$ \\Delta w_i \\propto -\\frac{\\partial E}{ \\partial w_i}$$\n",
    "\n",
    "$$ \\Delta w_i = -\\eta \\frac{\\partial E}{ \\partial w_i} $$\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "Using chain rule, we can see that :\n",
    "\n",
    "$$ \\frac{E}{\\partial w_i} = \\frac{\\partial}{\\partial w_i}\\frac{1}{2}(y-\\hat y)^2 = \\frac{\\partial}{\\partial w_i}\\frac{1}{2}(y-\\hat y(w_i))^2 = (y-\\hat y)\\frac{\\partial}{\\partial w_i}(y-\\hat y) \\\\ = -(y-\\hat y)\\frac{\\partial \\hat y}{\\partial w_i} = -(y-\\hat y)f'(h)\\frac{\\partial}{\\partial w_i}\\sum w_ix_i = -(y-\\hat y)f'(h)x_i $$\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "Now we get the weight update :\n",
    "\n",
    "$$ \\Delta w_i = \\eta(y-\\hat y)f'(h)x_i $$\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "To make things easier, we define the error term :\n",
    "\n",
    "$$ \\sigma = (y-\\hat y)f'(h) $$ \n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "So the weights can be updated by :\n",
    "\n",
    "$$ w_i = w_i + \\eta\\sigma x_i $$\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "We can easily extend this to neural net with multiple neurons:\n",
    "\n",
    "<img src=\"assets/neural_net_multi.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "<img src=\"assets/error_multi.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">\n",
    "For sigmoid function,\n",
    "\n",
    "$$ f(x) = \\frac {1}{1 + e^{-x}} $$\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "the derivative is :\n",
    "\n",
    "$$ f'(x) = -(1+e^{-x})^{-2}\\cdot e^{-x}\\cdot (-1 ) = \\frac{1}{1=e^{-x}}\\frac{1+e^{-x}-1}{1=e^{-x}} = f(x)(1-f(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"\n",
    "    # Derivative of the sigmoid function\n",
    "    \"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "learnrate = 0.5\n",
    "x = np.array([1, 2, 3, 4])\n",
    "y = np.array(0.5)\n",
    "\n",
    "# Initial weights\n",
    "w = np.array([0.5, -0.5, 0.3, 0.1])\n",
    "\n",
    "### Calculate one gradient descent step for each weight\n",
    "### Note: Some steps have been consilated, so there are\n",
    "###       fewer variable names than in the above sample code\n",
    "\n",
    "# TODO: Calculate the node's linear combination of inputs and weights\n",
    "h = \n",
    "\n",
    "# TODO: Calculate output of neural network\n",
    "nn_output = \n",
    "\n",
    "# TODO: Calculate error of neural network\n",
    "error = \n",
    "\n",
    "# TODO: Calculate the error term\n",
    "#       Remember, this requires the output gradient, which we haven't\n",
    "#       specifically added a variable for.\n",
    "error_term = \n",
    "# Note: The sigmoid_prime function calculates sigmoid(h) twice,\n",
    "#       but you've already calculated it once. You can make this\n",
    "#       code more efficient by calculating the derivative directly\n",
    "#       rather than calling sigmoid_prime, like this:\n",
    "# error_term = error * nn_output * (1 - nn_output)\n",
    "\n",
    "# TODO: Calculate change in weights\n",
    "del_w = \n",
    "\n",
    "print('Neural Network output:')\n",
    "print(nn_output)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do it right, you should see :   \n",
    "Neural Network output:     \n",
    "0.6899744811276125   \n",
    "Amount of Error:   \n",
    "-0.1899744811276125   \n",
    "Change in Weights:   \n",
    "[-0.02031869 -0.04063738 -0.06095608 -0.08127477]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
